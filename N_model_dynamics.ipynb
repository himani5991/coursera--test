{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himani5991/coursera--test/blob/master/N_model_dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjrlDIAy2dAJ"
      },
      "source": [
        "In this part we \n",
        " - encode dynamics of N-model (see description in \"Problem formulation (N model).pdf\"),\n",
        " - find the best threshold policy,\n",
        " - code and play with the REINFORCE policy gradient algorithm,\n",
        " - fix a policy and implement a value NN approximator,\n",
        " - write an actor-critic algorithm.\n",
        "\n",
        "\n",
        "The dynamics of the \"uniformized\" N model is provided next.\n",
        "\n",
        "Class `ProcessingNetwork` provides an interface for N-model formulation. \n",
        "\n",
        "Method `Nmodel_from_load` allows to create an N-model instance with a given load. \n",
        "\n",
        "Method `next_state_N1` generates the next state of the system given the current state and the action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lw9MzbQRSv-"
      },
      "outputs": [],
      "source": [
        "import random as r\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3PccvbnQJkf"
      },
      "outputs": [],
      "source": [
        "network_dictionary = {\n",
        "    'Nmodel': {\n",
        "        'A': [[-1, 0], [0, -1]],\n",
        "        'D': [[1, 1]],\n",
        "        'alpha': [1.3 * 0.95, 0.4 * 0.95],\n",
        "        'mu': [1, 1. / 2, 1],\n",
        "        'name': 'N-model',\n",
        "        'holding_cost': [3., 1.]}\n",
        "}\n",
        "\n",
        "\n",
        "class ProcessingNetwork:\n",
        "    # N-model network class\n",
        "    def __init__(self, A, D, alpha, mu, holding_cost,  name):\n",
        "        self.alpha = np.asarray(alpha)  # arrival rates\n",
        "        self.mu = np.asarray(mu)  # service rates\n",
        "        self.uniform_rate = np.sum(alpha)+np.sum(mu)  # uniform rate for uniformization\n",
        "        self.p_arriving = np.divide(self.alpha, self.uniform_rate)# normalized arrival rates\n",
        "        self.p_compl = np.divide(self.mu, self.uniform_rate) #normalized service rates\n",
        "        self.cumsum_rates = np.unique(np.cumsum(np.concatenate([self.p_arriving, self.p_compl])))\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def Nmodel_from_load(cls, load: float):\n",
        "        # another constructor for the standard queuing networks\n",
        "        # based on a queuing network name, find the queuing network info in the 'network_dictionary.py'\n",
        "        return cls(A=network_dictionary['Nmodel']['A'],\n",
        "                   D=network_dictionary['Nmodel']['D'],\n",
        "                   alpha=np.asarray([1.3 * load, 0.4 * load]),\n",
        "                   mu=network_dictionary['Nmodel']['mu'],\n",
        "                   holding_cost=network_dictionary['Nmodel']['holding_cost'],\n",
        "                   name=network_dictionary['Nmodel']['name'])\n",
        "\n",
        "    def next_state_N1(self, state, action):\n",
        "        \"\"\"\n",
        "        :param state: current state\n",
        "        :param action: [1, 0] if class 1 is prioritized; [0, 1] if class 2 is prioritized.\n",
        "        :return: next state\n",
        "        \"\"\"\n",
        "\n",
        "        w = np.random.random()\n",
        "        wi = 0 \n",
        "        while w > self.cumsum_rates[wi]: #randomly choose which state to go to \n",
        "            wi += 1\n",
        "        if wi == 0:\n",
        "            state_next = state + np.array([1, 0]) #station 1 new arrival \n",
        "        elif wi == 1:\n",
        "            state_next = state + np.array([0, 1]) #station 2 new arrival\n",
        "        elif wi == 2 and (state[0] > 0):\n",
        "            state_next = state - np.array([1, 0]) #station 1 departure \n",
        "        elif wi == 3 and ((action[0] == 1 or state[1] == 0) and state[0] > 1):\n",
        "            state_next = state - np.array([1, 0]) #station 1 departure\n",
        "        elif wi == 4 and ((action[0] == 0 or state[0] < 2) and state[1] > 0):\n",
        "            state_next = state - np.array([0, 1]) #station 2 departure \n",
        "        else:\n",
        "            state_next = state #no change \n",
        "        return state_next"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define an instance of the N model with load $\\rho=0.95$."
      ],
      "metadata": {
        "id": "5OpbHL0U4dQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4sAp3pMR1EC"
      },
      "outputs": [],
      "source": [
        "n_model_095 = ProcessingNetwork.Nmodel_from_load(load = 0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example how next state is generated, given current state and action."
      ],
      "metadata": {
        "id": "Y00w1elY5ArQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUasMVMFSp-q"
      },
      "outputs": [],
      "source": [
        " # state [1, 1] means there is one class 1 job and one class 2 job in the system; \n",
        " # action [1, 0] means class 1 jobs have priority. Another feasible action is [0, 1] that means that class 2 jobs have priority. \n",
        "print(n_model_095.next_state_N1(state = [1, 1], action = [0, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that always prioritizing class 1 job is a bad idea. "
      ],
      "metadata": {
        "id": "PZa6vwv36GC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = np.array([0, 0])\n",
        "for t in range(10**5):\n",
        "  state = n_model_095.next_state_N1(state, action = [1, 0])\n",
        "  print('time step:', t,' ', '# of jobs in the system:', np.sum(state))"
      ],
      "metadata": {
        "id": "kkrkmMdY6jVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The N-model system with load $\\rho=0.95$ is difficult to optimize. We decrease the load to make the control problem easier."
      ],
      "metadata": {
        "id": "Qa8gHhv_7Vxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_model_085 = ProcessingNetwork.Nmodel_from_load(load = 0.85)"
      ],
      "metadata": {
        "id": "zVLgEQ9y7srv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: threshold policy\n",
        "\n",
        "Find the best threshold policy for the N-model with load $\\rho=0.85$. Server 2 of the N-model operating under the threshold policy gives priority to\n",
        "class 1 jobs, if the number of class 1 jobs in the system is larger than a fixed threshold $T\\in \\mathbb{R}$.\n",
        "\n",
        "- For each fixed $T$, run for at least $10^5$ time-steps, compute the average cost. Recall, one-step cost of state $x = (x_1, x_2)$ is $3x_1+x_2$."
      ],
      "metadata": {
        "id": "WwSJhxnC42cM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd3MZeZqaXOj"
      },
      "outputs": [],
      "source": [
        "## your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best threshold policy is our benchmark now. We should try a policy-gradient algorithm to find a better policy."
      ],
      "metadata": {
        "id": "sZY4S_619tIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: policy gradient implementation.\n",
        "\n",
        "Algorithm pseudocode:\n",
        "- Initialize $z_0$, $\\hat \\eta$, learning rate $\\alpha_t$, $\\beta>0$\n",
        "- For each transition step $t=0, ..., T$ do\n",
        "  - Update average cost estimate\n",
        "$$\n",
        "\\hat \\eta_{t+1} = \\hat \\eta_t+\\beta \\alpha_t (c(x_t) - \\hat \\eta_t)\n",
        "$$\n",
        "  - If $x_t = x^*$ then $z_{t+1} = 0$, otherwise \n",
        "  $$\n",
        "   z_{t+1} = z_t +\\nabla_\\theta \\log \\pi (a_t|x_t)\n",
        "  $$\n",
        "  - Update parameters\n",
        "  $$\\theta_{t+1} = \\theta_t - \\alpha_t(c(x_t) - \\hat \\eta_t)z_t$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l0SBRCS7-0s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. Define a policy parameterization. A good choice is \n",
        "$$\n",
        "\\pi_\\theta(\\text{priority to class }1| x) = \\frac{1}{1+\\exp(\\theta_1x_1+\\theta_2x_2 +b)}, \n",
        "$$\n",
        "where $\\theta = (\\theta_1, \\theta_2)$ are parameters we want to optimize, $b\\in \\mathbb{R}$.\n",
        "\n",
        "Hint: $$\\pi_\\theta(\\text{priority to class }1| x) \\geq 1/2\\quad \\text{ if }\\quad b \\leq -\\theta_1x_1-\\theta_2x_2.$$\n",
        "\n",
        "Step 2. We need to be able to compute $\\nabla\\log \\pi_\\theta(a|x) = \\frac{\\nabla\\pi_\\theta(a|x)}{\\pi_\\theta(a|x)}$.\n",
        "\n",
        "Other hints: \n",
        "- $\\pi_\\theta(\\text{priority to class }2| x) = 1 - \\pi_\\theta(\\text{priority to class }1| x)$\n",
        "- $\\nabla_\\theta \\pi_\\theta(\\text{priority to class }2| x) =  - \\nabla_\\theta \\pi_\\theta(\\text{priority to class }1| x)$"
      ],
      "metadata": {
        "id": "IleeK8BmGF9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v46Q5qKatTeO"
      },
      "outputs": [],
      "source": [
        "## your code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Define the recurrent state, intial state, initial average cost, number of steps, etc.\n",
        "\n",
        "Step 4. Implement the algorithm. Try different parametrizations, initial states, learning rates, etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6m3jrc8DFcK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w6qwMPtmwT2r"
      },
      "outputs": [],
      "source": [
        "## your code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Implement \"value\" neural network for a fixed policy.\n",
        "\n",
        "Step 1. Read an example of how a feed-forward neural network is created in TensorFlow: https://colab.research.google.com/drive/1xPIZzztD5XG0RcLvsP9Drs3puFCWGAOo?usp=sharing\n",
        "\n",
        "Step 2. Let's consider the threshold policy $T=5$. Generate $N=5000$ regenerative cycles under this policy. Transitions from $x^*$ to itself counts as cycles. We have trajectory data:\n",
        "$$\n",
        "(x_0=x^*, a_0, x_1, a_1, x_2, ...., x_{\\sigma_N-1}),\n",
        "$$\n",
        "where $\\sigma_n$ is the $n$th time the regenerative state $x^*$ is visited.\n",
        "\n",
        "Step 3. Estimate the average cost. Recall,\n",
        "$$\n",
        "\\eta = \\frac{1}{\\mathbb{E}[\\sigma(x^*)]} \\mathbb{E}\\left[\\sum\\limits_{t=0}^{\\sigma(x^*)-1}c(x_t)~|~x_0=x\\right].\n",
        "$$\n",
        "Hence, its estimate is\n",
        "$$\n",
        "\\hat \\eta = \\frac{1}{\\sigma_N}\\sum\\limits_{t=0}^{\\sigma_N-1}c(x_t).\n",
        "$$\n",
        "\n",
        "Step 4. Compute one-replication estimates of the value function.\n",
        "\n",
        "The threshold policy $T=5$ has its (relative) value function:\n",
        "$$\n",
        "V^{\\pi}(x):=\\mathbb{E}\\left[\\sum\\limits_{t=0}^{\\sigma(x^*)-1} (c(x_t) - \\eta^\\pi) ~|~x_0=x\\right].\n",
        "$$\n",
        "\n",
        "We have trajectory data $(x_0, a_1, x_1, a_2, ..., x_{\\sigma_N-1})$ generated  under this policy. For each state in this trajectory, we can compute one-replication estimate of the value function. For example, for a state $x_t$ visited at step $t$:\n",
        "$$\n",
        "\\hat V_t:=\\sum\\limits_{k=t}^{\\sigma(t)-1} (c(x_k) - \\hat \\eta),\n",
        "$$\n",
        "where $\\sigma(t) = \\min\\{k>t~|~x_k=x^*\\}$.\n",
        "\n",
        "We note that the one-replication estimate is computed for every timestep.\n",
        "\n",
        "Step 5. We define $f_\\phi$, $\\phi\\in \\Phi$, as a \"value\" neural network. We want to find parameters $\\phi^*$ s.t. \n",
        "$$\n",
        "\\phi^* = \\arg\\min\\limits_{\\phi \\in \\Phi}\\sum\\limits_{t=0}^{\\sigma_N-1} \\left(f_\\phi(x_t) - \\hat V_t\\right)^2.\n",
        "$$\n",
        "\n",
        "Please, note, there is no need to average one-replication estimates that correspond to one state:\n",
        "$$\n",
        "\\arg\\min\\limits_{x\\in B} \\sum\\limits_{i=1}^n\\left(x-a_i\\right)^2 = \\arg\\min\\limits_{x\\in B} \\left(x-\\frac{1}{n}\\sum\\limits_{i=1}^na_i\\right)^2, \n",
        "$$\n",
        "where $B$ is an arbitrary subset of $\\mathbb{R}$."
      ],
      "metadata": {
        "id": "I3uQ6HufkjAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 5\n",
        "N = 5000\n",
        "\n",
        "## collect data\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "Ri-DXOgI7XKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## estimate the average cost"
      ],
      "metadata": {
        "id": "Xt_gdH_m-TEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## estimate the value function"
      ],
      "metadata": {
        "id": "lZr979NL-gno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define NN and optimize its parameters"
      ],
      "metadata": {
        "id": "qxSipkHUDIZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 (Optional).  Implement an actor-critic algorithm using 'value' neural network as a critic. \n",
        "\n",
        "See slide 46 for the pseudecode of the algorithm."
      ],
      "metadata": {
        "id": "EaBTiJ8YHodP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code"
      ],
      "metadata": {
        "id": "GtRWxOSykdHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "N-model dynamics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}